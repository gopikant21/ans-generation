{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DjTdIsq23AU",
        "outputId": "113eefe2-92cf-418b-db91-86c9641a9b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.74 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting openai<2.0.0,>=1.99.9 (from langchain_openai)\n",
            "  Downloading openai-1.99.9-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.99.9-py3-none-any.whl (786 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai, langchain-core, langchain_openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.99.1\n",
            "    Uninstalling openai-1.99.1:\n",
            "      Successfully uninstalled openai-1.99.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.72\n",
            "    Uninstalling langchain-core-0.3.72:\n",
            "      Successfully uninstalled langchain-core-0.3.72\n",
            "Successfully installed langchain-core-0.3.74 langchain_openai-0.3.30 openai-1.99.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGDglJYJyzKS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "class EvaluationResult(BaseModel):\n",
        "    relevance: float = Field(..., description=\"Relevance score between 0 and 100\")\n",
        "    factual_accuracy: float = Field(..., description=\"Accuracy score between 0 and 100\")\n",
        "    completeness: float = Field(..., description=\"Completeness score between 0 and 100\")\n",
        "    overall_score: float = Field(..., description=\"Weighted score between 0 and 100\")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\", \"references\", \"student_answer\"],\n",
        "    template=\"\"\"\n",
        "You are an evaluator. Compare the STUDENT ANSWER to the REFERENCE if reference is available or else evaluate yourself.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Reference Answers:\n",
        "{references}\n",
        "\n",
        "Student Answer:\n",
        "{student_answer}\n",
        "\n",
        "Scoring rules:\n",
        "- relevance: between 0 and 100\n",
        "- factual_accuracy: between 0 and 100\n",
        "- completeness: between 0 and 100\n",
        "Formula: overall_score = 0.35 * factual_accuracy + 0.45 * relevance + 0.2 * completeness\n",
        "\n",
        "Respond ONLY with valid JSON in this format:\n",
        "{{\n",
        "  \"relevance\": <float>,\n",
        "  \"factual_accuracy\": <float>,\n",
        "  \"completeness\": <float>,\n",
        "  \"overall_score\": <float>\n",
        "}}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "def extract_json(text: str) -> dict:\n",
        "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
        "    if not match:\n",
        "        raise ValueError(f\"No JSON found in model output: {text}\")\n",
        "    json_str = match.group(0)\n",
        "    return json.loads(json_str)\n",
        "\n",
        "def evaluate(question: str, references: List[str], student_answer: str) -> dict:\n",
        "    prompt = prompt_template.format(\n",
        "        question=question,\n",
        "        references=\"\\n\".join(references),\n",
        "        student_answer=student_answer\n",
        "    )\n",
        "    response = llm.invoke(prompt)\n",
        "    response_text = response.content if hasattr(response, \"content\") else str(response)\n",
        "    result_dict = extract_json(response_text)\n",
        "    validated = EvaluationResult(**result_dict)\n",
        "    return validated.model_dump()  # Fixed: Use model_dump() instead of dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "842JKqIE7nLA",
        "outputId": "df1e224f-6d5a-4055-cb82-d40317867474"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"relevance\": 30.0,\n",
            "  \"factual_accuracy\": 40.0,\n",
            "  \"completeness\": 5.0,\n",
            "  \"overall_score\": 28.5\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"relevance\": 40.0,\n",
            "  \"factual_accuracy\": 10.0,\n",
            "  \"completeness\": 10.0,\n",
            "  \"overall_score\": 23.5\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"relevance\": 90.0,\n",
            "  \"factual_accuracy\": 80.0,\n",
            "  \"completeness\": 20.0,\n",
            "  \"overall_score\": 72.5\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"relevance\": 100.0,\n",
            "  \"factual_accuracy\": 95.0,\n",
            "  \"completeness\": 40.0,\n",
            "  \"overall_score\": 86.25\n",
            "}\n",
            "{\n",
            "  \"relevance\": 100.0,\n",
            "  \"factual_accuracy\": 95.0,\n",
            "  \"completeness\": 75.0,\n",
            "  \"overall_score\": 93.25\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        }
      ],
      "source": [
        "q = \"What is p-value in statistics?\"\n",
        "\n",
        "refs = [\"The p-value is a fundamental concept in statistical hypothesis testing, used to quantify the strength of evidence against a null hypothesis. It is defined as the probability of obtaining a test statistic at least as extreme as the one actually observed, assuming the null hypothesis is true. In other words, it answers the question: ‚ÄúIf the null hypothesis were correct, how likely is it that we would see results like this (or more extreme) purely by random chance?\"\n",
        "\"A small p-value (commonly less than a chosen significance level, such as 0.05) suggests that the observed data would be quite rare if the null hypothesis were true. This provides evidence in favor of the alternative hypothesis, leading researchers to reject the null. Conversely, a large p-value indicates that the observed data is consistent with the null hypothesis, and there is insufficient evidence to reject it. Importantly, the p-value does not measure the probability that the null hypothesis itself is true, nor does it indicate the size or practical importance of an effect ‚Äî it only assesses the compatibility between the data and the null model.\"\n",
        "\"In practice, p-values are used alongside effect sizes, confidence intervals, and subject-matter knowledge to make informed conclusions. While widely used, p-values can be misinterpreted and should not be the sole basis for decision-making; they are best understood as one piece of the broader statistical inference process.\"]\n",
        "\n",
        "s0 = \"p-value is probability\"\n",
        "s1 = \"Area under the distribution curve outside the confidence interval is the p-value\"\n",
        "s2 = \"Probability of an event that is atleast as extreme as the observed event is known as p-value\"\n",
        "s3 = \"p-value measures the probability of an event that is atleast as extreme as the observed event, assuming null hypothesis is true\"\n",
        "s4 = \"The p-value is a measure of how compatible your observed data is with the assumption that the null hypothesis is correct ‚Äî it represents the likelihood of seeing the observed effect or something more extreme purely due to random chance if the null hypothesis holds.\"\n",
        "\n",
        "student = [s0, s1, s2, s3, s4]\n",
        "\n",
        "for i in range(len(student)):\n",
        "  print(json.dumps(evaluate(q, refs, student[i]), indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt7p_BxpalTb"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Test of Automated Evaluation - First 10 Questions\n",
        "import json\n",
        "\n",
        "# Load the response file\n",
        "with open(\"response_1755348961176.json\", 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "print(f\"Loaded {len(data['results'])} questions from the response file\")\n",
        "print(f\"Testing evaluation on first 10 questions with ALL candidate answers\")\n",
        "\n",
        "# Test evaluation on first 10 questions with all candidate answers\n",
        "test_results = []\n",
        "questions_processed = 0\n",
        "total_answers_evaluated = 0\n",
        "\n",
        "for idx, result in enumerate(data['results'][:10]):  # Test first 10 questions\n",
        "    question = result['question']\n",
        "    domain = result.get('domain', 'Unknown')\n",
        "    difficulty = result.get('difficulty', 'Unknown')\n",
        "    \n",
        "    # Check if reference answers exist\n",
        "    reference_answers = []\n",
        "    if 'reference_answers' in result and result['reference_answers']:\n",
        "        for ref in result['reference_answers']:\n",
        "            if isinstance(ref, dict) and 'answer' in ref:\n",
        "                reference_answers.append(ref['answer'])\n",
        "    \n",
        "    if not reference_answers:\n",
        "        print(f\"Skipping question {idx+1}: No reference answers\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing Question {idx+1}:\")\n",
        "    print(f\"Domain: {domain}, Difficulty: {difficulty}\")\n",
        "    print(f\"Question: {question[:100]}...\")\n",
        "    print(f\"Reference answers available: {len(reference_answers)}\")\n",
        "    \n",
        "    # Evaluate ALL candidate answers for this question\n",
        "    if 'candidate_answers' in result and result['candidate_answers']:\n",
        "        print(f\"Candidate answers to evaluate: {len(result['candidate_answers'])}\")\n",
        "        \n",
        "        question_scores = []\n",
        "        for candidate_idx, candidate_answer in enumerate(result['candidate_answers']):\n",
        "            if isinstance(candidate_answer, dict):\n",
        "                answer_text = candidate_answer.get('answer', '')\n",
        "                answer_type = candidate_answer.get('type', 'unknown')\n",
        "            else:\n",
        "                answer_text = candidate_answer\n",
        "                answer_type = 'unknown'\n",
        "            \n",
        "            print(f\"\\n  Candidate {candidate_idx + 1}/{len(result['candidate_answers'])}:\")\n",
        "            print(f\"  Type: {answer_type}\")\n",
        "            print(f\"  Answer preview: {answer_text[:120]}...\")\n",
        "            \n",
        "            try:\n",
        "                # Use the existing evaluate function\n",
        "                evaluation = evaluate(question, reference_answers, answer_text)\n",
        "                evaluation['question_index'] = idx + 1\n",
        "                evaluation['domain'] = domain\n",
        "                evaluation['difficulty'] = difficulty\n",
        "                evaluation['answer_type'] = answer_type\n",
        "                evaluation['candidate_index'] = candidate_idx + 1\n",
        "                \n",
        "                test_results.append(evaluation)\n",
        "                question_scores.append(evaluation['overall_score'])\n",
        "                total_answers_evaluated += 1\n",
        "                \n",
        "                print(f\"  ‚úÖ Evaluation completed:\")\n",
        "                print(f\"     Overall Score: {evaluation['overall_score']:.1f}\")\n",
        "                print(f\"     Relevance: {evaluation['relevance']:.1f}\")\n",
        "                print(f\"     Accuracy: {evaluation['factual_accuracy']:.1f}\")\n",
        "                print(f\"     Completeness: {evaluation['completeness']:.1f}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Evaluation failed: {e}\")\n",
        "        \n",
        "        # Show question-level summary\n",
        "        if question_scores:\n",
        "            avg_score = sum(question_scores) / len(question_scores)\n",
        "            best_score = max(question_scores)\n",
        "            worst_score = min(question_scores)\n",
        "            print(f\"\\n  Question {idx+1} Summary:\")\n",
        "            print(f\"    Answers evaluated: {len(question_scores)}\")\n",
        "            print(f\"    Average score: {avg_score:.1f}\")\n",
        "            print(f\"    Best score: {best_score:.1f}\")\n",
        "            print(f\"    Worst score: {worst_score:.1f}\")\n",
        "        \n",
        "        questions_processed += 1\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"COMPREHENSIVE TEST EVALUATION SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Questions processed: {questions_processed}\")\n",
        "print(f\"Total answers evaluated: {total_answers_evaluated}\")\n",
        "\n",
        "if test_results:\n",
        "    # Overall statistics\n",
        "    avg_overall = sum(r['overall_score'] for r in test_results) / len(test_results)\n",
        "    avg_relevance = sum(r['relevance'] for r in test_results) / len(test_results)\n",
        "    avg_accuracy = sum(r['factual_accuracy'] for r in test_results) / len(test_results)\n",
        "    avg_completeness = sum(r['completeness'] for r in test_results) / len(test_results)\n",
        "    \n",
        "    best_score = max(r['overall_score'] for r in test_results)\n",
        "    worst_score = min(r['overall_score'] for r in test_results)\n",
        "    \n",
        "    print(f\"\\nOverall Performance:\")\n",
        "    print(f\"  Average Overall Score: {avg_overall:.2f}\")\n",
        "    print(f\"  Average Relevance: {avg_relevance:.2f}\")\n",
        "    print(f\"  Average Factual Accuracy: {avg_accuracy:.2f}\")\n",
        "    print(f\"  Average Completeness: {avg_completeness:.2f}\")\n",
        "    print(f\"  Score Range: {worst_score:.1f} - {best_score:.1f}\")\n",
        "    \n",
        "    # Performance by domain\n",
        "    domains = {}\n",
        "    for result in test_results:\n",
        "        domain = result['domain']\n",
        "        if domain not in domains:\n",
        "            domains[domain] = []\n",
        "        domains[domain].append(result['overall_score'])\n",
        "    \n",
        "    print(f\"\\nPerformance by Domain:\")\n",
        "    for domain, scores in domains.items():\n",
        "        avg_domain_score = sum(scores) / len(scores)\n",
        "        print(f\"  {domain}: {avg_domain_score:.2f} (n={len(scores)})\")\n",
        "    \n",
        "    # Performance by difficulty\n",
        "    difficulties = {}\n",
        "    for result in test_results:\n",
        "        difficulty = result['difficulty']\n",
        "        if difficulty not in difficulties:\n",
        "            difficulties[difficulty] = []\n",
        "        difficulties[difficulty].append(result['overall_score'])\n",
        "    \n",
        "    print(f\"\\nPerformance by Difficulty:\")\n",
        "    for difficulty, scores in difficulties.items():\n",
        "        avg_difficulty_score = sum(scores) / len(scores)\n",
        "        print(f\"  {difficulty}: {avg_difficulty_score:.2f} (n={len(scores)})\")\n",
        "    \n",
        "    # Performance by answer type\n",
        "    answer_types = {}\n",
        "    for result in test_results:\n",
        "        answer_type = result.get('answer_type', 'unknown')\n",
        "        if answer_type not in answer_types:\n",
        "            answer_types[answer_type] = []\n",
        "        answer_types[answer_type].append(result['overall_score'])\n",
        "    \n",
        "    print(f\"\\nPerformance by Answer Type:\")\n",
        "    for answer_type, scores in sorted(answer_types.items(), key=lambda x: sum(x[1])/len(x[1]), reverse=True):\n",
        "        avg_type_score = sum(scores) / len(scores)\n",
        "        print(f\"  {answer_type}: {avg_type_score:.2f} (n={len(scores)})\")\n",
        "    \n",
        "    # Top and bottom performers\n",
        "    sorted_results = sorted(test_results, key=lambda x: x['overall_score'], reverse=True)\n",
        "    \n",
        "    print(f\"\\nTop 5 Performers:\")\n",
        "    for i, result in enumerate(sorted_results[:5], 1):\n",
        "        print(f\"  {i}. Q{result['question_index']}-C{result['candidate_index']} ({result['domain']}): {result['overall_score']:.1f} - {result['answer_type']}\")\n",
        "    \n",
        "    print(f\"\\nBottom 5 Performers:\")\n",
        "    for i, result in enumerate(sorted_results[-5:], 1):\n",
        "        print(f\"  {i}. Q{result['question_index']}-C{result['candidate_index']} ({result['domain']}): {result['overall_score']:.1f} - {result['answer_type']}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Comprehensive automated evaluation system is working!\")\n",
        "print(f\"üí° This demonstrates the full evaluation capability\")\n",
        "print(f\"üöÄ Ready to run on the complete dataset using evaluation_runner.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Pydantic V2 Compatibility Update\n",
        "\n",
        "**Fixed Deprecation Warning**: Updated the evaluation functions to use `model_dump()` instead of the deprecated `dict()` method for Pydantic V2 compatibility.\n",
        "\n",
        "### Changes Made:\n",
        "- **Cell 2**: Updated `evaluate()` function to use `validated.model_dump()`\n",
        "- **automated_evaluation.py**: Updated `evaluate_single_answer()` method\n",
        "- **evaluation_runner.py**: Already compatible with Pydantic V2\n",
        "\n",
        "This ensures compatibility with current and future versions of Pydantic while maintaining the same functionality."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
