{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DjTdIsq23AU",
        "outputId": "113eefe2-92cf-418b-db91-86c9641a9b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.74 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting openai<2.0.0,>=1.99.9 (from langchain_openai)\n",
            "  Downloading openai-1.99.9-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.99.9-py3-none-any.whl (786 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai, langchain-core, langchain_openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.99.1\n",
            "    Uninstalling openai-1.99.1:\n",
            "      Successfully uninstalled openai-1.99.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.72\n",
            "    Uninstalling langchain-core-0.3.72:\n",
            "      Successfully uninstalled langchain-core-0.3.72\n",
            "Successfully installed langchain-core-0.3.74 langchain_openai-0.3.30 openai-1.99.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGDglJYJyzKS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "class EvaluationResult(BaseModel):\n",
        "    relevance: float = Field(..., description=\"Relevance score between 0 and 100\")\n",
        "    factual_accuracy: float = Field(..., description=\"Accuracy score between 0 and 100\")\n",
        "    completeness: float = Field(..., description=\"Completeness score between 0 and 100\")\n",
        "    overall_score: float = Field(..., description=\"Weighted score between 0 and 100\")\n",
        "\n",
        "llm = ChatOpenAI(model=\"o4-mini\")\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\", \"references\", \"student_answer\"],\n",
        "    template=\"\"\"\n",
        "You are an evaluator. Compare the STUDENT ANSWER to the REFERENCE if reference is available or else evaluate yourself.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Reference Answers:\n",
        "{references}\n",
        "\n",
        "Student Answer:\n",
        "{student_answer}\n",
        "\n",
        "Scoring rules:\n",
        "- relevance: between 0 and 100\n",
        "- factual_accuracy: between 0 and 100\n",
        "- completeness: between 0 and 100\n",
        "Formula: overall_score = 0.35 * factual_accuracy + 0.45 * relevance + 0.2 * completeness\n",
        "\n",
        "Respond ONLY with valid JSON in this format:\n",
        "{{\n",
        "  \"relevance\": <float>,\n",
        "  \"factual_accuracy\": <float>,\n",
        "  \"completeness\": <float>,\n",
        "  \"overall_score\": <float>\n",
        "}}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "def extract_json(text: str) -> dict:\n",
        "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
        "    if not match:\n",
        "        raise ValueError(f\"No JSON found in model output: {text}\")\n",
        "    json_str = match.group(0)\n",
        "    return json.loads(json_str)\n",
        "\n",
        "def evaluate(question: str, references: List[str], student_answer: str) -> dict:\n",
        "    prompt = prompt_template.format(\n",
        "        question=question,\n",
        "        references=\"\\n\".join(references),\n",
        "        student_answer=student_answer\n",
        "    )\n",
        "    response = llm.invoke(prompt)\n",
        "    response_text = response.content if hasattr(response, \"content\") else str(response)\n",
        "    result_dict = extract_json(response_text)\n",
        "    validated = EvaluationResult(**result_dict)\n",
        "    return validated.dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "842JKqIE7nLA",
        "outputId": "df1e224f-6d5a-4055-cb82-d40317867474"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"relevance\": 30.0,\n",
            "  \"factual_accuracy\": 40.0,\n",
            "  \"completeness\": 5.0,\n",
            "  \"overall_score\": 28.5\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"relevance\": 40.0,\n",
            "  \"factual_accuracy\": 10.0,\n",
            "  \"completeness\": 10.0,\n",
            "  \"overall_score\": 23.5\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"relevance\": 90.0,\n",
            "  \"factual_accuracy\": 80.0,\n",
            "  \"completeness\": 20.0,\n",
            "  \"overall_score\": 72.5\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"relevance\": 100.0,\n",
            "  \"factual_accuracy\": 95.0,\n",
            "  \"completeness\": 40.0,\n",
            "  \"overall_score\": 86.25\n",
            "}\n",
            "{\n",
            "  \"relevance\": 100.0,\n",
            "  \"factual_accuracy\": 95.0,\n",
            "  \"completeness\": 75.0,\n",
            "  \"overall_score\": 93.25\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-43586821.py:66: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return validated.dict()\n"
          ]
        }
      ],
      "source": [
        "q = \"What is p-value in statistics?\"\n",
        "\n",
        "refs = [\"The p-value is a fundamental concept in statistical hypothesis testing, used to quantify the strength of evidence against a null hypothesis. It is defined as the probability of obtaining a test statistic at least as extreme as the one actually observed, assuming the null hypothesis is true. In other words, it answers the question: “If the null hypothesis were correct, how likely is it that we would see results like this (or more extreme) purely by random chance?\"\n",
        "\"A small p-value (commonly less than a chosen significance level, such as 0.05) suggests that the observed data would be quite rare if the null hypothesis were true. This provides evidence in favor of the alternative hypothesis, leading researchers to reject the null. Conversely, a large p-value indicates that the observed data is consistent with the null hypothesis, and there is insufficient evidence to reject it. Importantly, the p-value does not measure the probability that the null hypothesis itself is true, nor does it indicate the size or practical importance of an effect — it only assesses the compatibility between the data and the null model.\"\n",
        "\"In practice, p-values are used alongside effect sizes, confidence intervals, and subject-matter knowledge to make informed conclusions. While widely used, p-values can be misinterpreted and should not be the sole basis for decision-making; they are best understood as one piece of the broader statistical inference process.\"]\n",
        "\n",
        "s0 = \"p-value is probability\"\n",
        "s1 = \"Area under the distribution curve outside the confidence interval is the p-value\"\n",
        "s2 = \"Probability of an event that is atleast as extreme as the observed event is known as p-value\"\n",
        "s3 = \"p-value measures the probability of an event that is atleast as extreme as the observed event, assuming null hypothesis is true\"\n",
        "s4 = \"The p-value is a measure of how compatible your observed data is with the assumption that the null hypothesis is correct — it represents the likelihood of seeing the observed effect or something more extreme purely due to random chance if the null hypothesis holds.\"\n",
        "\n",
        "student = [s0, s1, s2, s3, s4]\n",
        "\n",
        "for i in range(len(student)):\n",
        "  print(json.dumps(evaluate(q, refs, student[i]), indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt7p_BxpalTb"
      },
      "outputs": [],
      "source": [
        "# Quick Test of Automated Evaluation\n",
        "import json\n",
        "\n",
        "# Load the response file\n",
        "with open(\"response_1755348961176.json\", 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "print(f\"Loaded {len(data['results'])} questions from the response file\")\n",
        "\n",
        "# Test evaluation on first question with available reference answers\n",
        "test_results = []\n",
        "questions_processed = 0\n",
        "\n",
        "for idx, result in enumerate(data['results'][:3]):  # Test first 3 questions only\n",
        "    question = result['question']\n",
        "    domain = result.get('domain', 'Unknown')\n",
        "    difficulty = result.get('difficulty', 'Unknown')\n",
        "    \n",
        "    # Check if reference answers exist\n",
        "    reference_answers = []\n",
        "    if 'reference_answers' in result and result['reference_answers']:\n",
        "        for ref in result['reference_answers']:\n",
        "            if isinstance(ref, dict) and 'answer' in ref:\n",
        "                reference_answers.append(ref['answer'])\n",
        "    \n",
        "    if not reference_answers:\n",
        "        print(f\"Skipping question {idx+1}: No reference answers\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\nProcessing Question {idx+1}:\")\n",
        "    print(f\"Domain: {domain}, Difficulty: {difficulty}\")\n",
        "    print(f\"Question: {question[:100]}...\")\n",
        "    \n",
        "    # Test with first candidate answer if available\n",
        "    if 'candidate_answers' in result and result['candidate_answers']:\n",
        "        candidate_answer = result['candidate_answers'][0]\n",
        "        if isinstance(candidate_answer, dict):\n",
        "            answer_text = candidate_answer.get('answer', '')\n",
        "            answer_type = candidate_answer.get('type', 'unknown')\n",
        "        else:\n",
        "            answer_text = candidate_answer\n",
        "            answer_type = 'unknown'\n",
        "        \n",
        "        print(f\"Evaluating answer type: {answer_type}\")\n",
        "        print(f\"Answer preview: {answer_text[:150]}...\")\n",
        "        \n",
        "        try:\n",
        "            # Use the existing evaluate function\n",
        "            evaluation = evaluate(question, reference_answers, answer_text)\n",
        "            evaluation['question_index'] = idx + 1\n",
        "            evaluation['domain'] = domain\n",
        "            evaluation['difficulty'] = difficulty\n",
        "            evaluation['answer_type'] = answer_type\n",
        "            \n",
        "            test_results.append(evaluation)\n",
        "            questions_processed += 1\n",
        "            \n",
        "            print(f\"✅ Evaluation completed:\")\n",
        "            print(f\"   Overall Score: {evaluation['overall_score']:.1f}\")\n",
        "            print(f\"   Relevance: {evaluation['relevance']:.1f}\")\n",
        "            print(f\"   Accuracy: {evaluation['factual_accuracy']:.1f}\")\n",
        "            print(f\"   Completeness: {evaluation['completeness']:.1f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Evaluation failed: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"TEST EVALUATION SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Questions processed: {questions_processed}\")\n",
        "\n",
        "if test_results:\n",
        "    avg_overall = sum(r['overall_score'] for r in test_results) / len(test_results)\n",
        "    avg_relevance = sum(r['relevance'] for r in test_results) / len(test_results)\n",
        "    avg_accuracy = sum(r['factual_accuracy'] for r in test_results) / len(test_results)\n",
        "    avg_completeness = sum(r['completeness'] for r in test_results) / len(test_results)\n",
        "    \n",
        "    print(f\"Average Overall Score: {avg_overall:.2f}\")\n",
        "    print(f\"Average Relevance: {avg_relevance:.2f}\")\n",
        "    print(f\"Average Factual Accuracy: {avg_accuracy:.2f}\")\n",
        "    print(f\"Average Completeness: {avg_completeness:.2f}\")\n",
        "    \n",
        "    # Show individual results\n",
        "    print(f\"\\nDetailed Results:\")\n",
        "    for i, result in enumerate(test_results, 1):\n",
        "        print(f\"  {i}. Domain: {result['domain']}, Score: {result['overall_score']:.1f}\")\n",
        "\n",
        "print(f\"\\n✅ Automated evaluation system is working!\")\n",
        "print(f\"💡 To evaluate all questions, run the full evaluation_runner.py script\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
